from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage
from django.conf import settings

class AIHelper:
    def __init__(self, session_id):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY
        )

        # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ì–µì„ ìœ ì§€í•˜ê¸° ìœ„í•´ memoryë¥¼ ì„¸ì…˜ë³„ ì €ì¥
        if session_id not in _advisor_instances:
            _advisor_instances[session_id] = {
                "memory": ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True
                )
            }
        
        self.memory = _advisor_instances[session_id]["memory"]

    def create_prompt(self, question, chat_history):
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        messages = [
            ("system", "ë‹¹ì‹ ì€ ì§ˆë¬¸ìì˜ ì¹œí•œ ì¹œêµ¬ì¼ìˆ˜ë„, ê°€ì¡±ì¼ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ì…ì¥ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
        ]

        # ğŸ”¹ ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        for chat in chat_history:
            if isinstance(chat, HumanMessage):
                messages.append(("human", chat.content))
            elif isinstance(chat, AIMessage):
                messages.append(("assistant", chat.content))

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append(("human", "{question}"))

        return ChatPromptTemplate.from_messages(messages)

    def generate_response(self, question):
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        # 1. ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        chat_history = self.memory.load_memory_variables({}).get("chat_history", [])
        if chat_history is None:
            chat_history = []

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.create_prompt(question, chat_history)
        
        # 3. ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
        chain = prompt | self.llm
        response = chain.invoke({"question": question})

        # 4. ëŒ€í™” ì €ì¥
        self.memory.save_context(
            {"input": question},
            {"output": str(response)}
        )
        
        return str(response)

# ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ì—­ìœ¼ë¡œ ìœ ì§€
_advisor_instances = {}

# ë©”ì¸ í•¨ìˆ˜
def ai_chat(question, session_id):
    if session_id not in _advisor_instances:
        _advisor_instances[session_id] = AIHelper(session_id)
    
    advisor = _advisor_instances[session_id]
    response = advisor.generate_response(question)
    
    return response
